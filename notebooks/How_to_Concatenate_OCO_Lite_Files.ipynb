{"cells":[{"cell_type":"markdown","metadata":{},"source":["# How to Concatenate OCO Lite Files Using Python\n","\n","### Overview\n","\n","This Jupyter Notebook demonstrates how to exploit remote OPeNDAP access and concatenate many [Orbiting Carbon Observatory-2 (OCO-2) Lite](https://disc.gsfc.nasa.gov/datasets/OCO2_L2_Lite_FP_11.1r/summary?keywords=OCO2_L2_Lite_FP) daily files into one data object, without downloading individual Lite files.\n","\n","### Prerequisites\n","\n","This notebook was written using Python 3.10, and requires these libraries and files:\n","\n","- `netrc` file with valid Earthdata Login credentials\n","   - [How to Generate Earthdata Prerequisite Files](https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files)\n","- [netCDF4-python](https://github.com/Unidata/netcdf4-python) (we recommend using version 1.6.2)\n","- [python-cmr](https://github.com/nasa/python_cmr)\n","- [NumPy](https://numpy.org/install/)\n","\n","#### Optional Anaconda Environment YAML:\n","\n","This notebook can be run using the ['opendap' YAML file](https://github.com/nasa/gesdisc-tutorials/tree/main/environments/opendap.yml) provided in the 'environments' subfolder.\n","\n","Please follow the instructions [here](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file) to install and activate this environment. "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os, sys\n","import datetime, time\n","import netCDF4 as nc\n","import numpy as np\n","from cmr import CollectionQuery, GranuleQuery, ToolQuery, ServiceQuery, VariableQuery"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Set the time period to concatenate."]},{"cell_type":"markdown","metadata":{},"source":["Note the ISO 8601 time format. It is critical to adhere to it.<br>\n","There are no checks to ensure the format is correct."]},{"cell_type":"markdown","metadata":{},"source":["These time strings are used in the CMR query, and if the format is not correct, null<br>\n","result will be received."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["t1 = datetime.datetime(2020,3,1)\n","t2 = datetime.datetime(2020,3,2,23,59)"]},{"cell_type":"markdown","metadata":{},"source":["###  2. Specify Earthdata dataset short name and dataset version strings."]},{"cell_type":"markdown","metadata":{},"source":["Make sure to use the currently available dataset version."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["short_name = 'OCO2_L2_Lite_FP'\n","vid = '11.1r'"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Define function for retrieving OPeNDAP URLs\n","\n","The function below will return a list of OPeNDAP URLs given a short name, version ID, start and end time, latitude/longitude points, or a bounding box."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_CMRgranurls(ShortName,VersionID,start_time,end_time,lon=None,lat=None,rad_km=None):\n","    \"\"\"\n","    This program returns a list of OPeNDAP URLs.  If lon, lat, and radius are given, this function will search for all granules defined by that circle.  If lon and lat are 2 element lists, then this function will search for all granules defined by a bounding box.  If lon, lat, and radius_km are not given this function will search for all granules between the input start_time and end_time.  This function uses python-cmr to get the urls but it does not require prior knowledge of how many urls will be returned.\n","INPUTS\n","    \"ShortName\" the data set shortname\n","    \"VersionID\" the version ID of the product\n","    \"start_time\"  start time in utc.  The following is appended to the date: \"T00:00:00Z.\n","    \"end_time\"  start time in utc The following is appended to the date: \"T00:00:00Z, thus the end date is not actualy included in the results.\n","    \"lon\", \"lat\", \"rad_km\" (optional) will search within a radius or bounding box near a given location. If a radius or bounding box is not given, this function will return all granules within the input timerange.  \n","    OUTPUTS\n","    \"urls\" A list containing URLs\n","     \n","EXAMPLE\n","    Find all of the AIRX2RET granules within 30 km of  New Orleans\n","    > from opensearchtools.get_MOSurls import get_MOSurls\n","    > urls = get_CMRgranurls('AIRX2RET.006','2002.09.01','2016.01.01',lon=-90.0667,lat=29.95, rad_km=30.0)\n","    If a radius is not specified it will return all of the granules on the days in the search period.  For     example, following will return 241 granules.  The 1 extra if from 2002.08.31 part of which is on 2002.09.01\n","    > \n","    > urls = get_CMRgranurls('AIRX2RET.006','2002.09.01','2002.09.02')\n","    HISTORY\n","    Created by Thomas Hearty,  March 14, 2016 \n","    \"\"\"\n","    start_time = start_time.replace('.','-')\n","    end_time = end_time.replace('.','-')\n","    if len(start_time) == 10:\n","        start_time = start_time+\"T00:00:00Z\"\n","        end_time = end_time+\"T00:00:00Z\"\n","\n","    # create a list of start and end times strings\n","    start_dt = datetime.datetime.fromtimestamp(time.mktime(time.strptime(start_time,\"%Y-%m-%dT%H:%M:%SZ\")))\n","    end_dt = datetime.datetime.fromtimestamp(time.mktime(time.strptime(end_time,\"%Y-%m-%dT%H:%M:%SZ\")))\n","    \n","    deltatime = end_dt - start_dt\n","    start_times = [start_time]\n","    end_times = []\n","    timeincrement = datetime.timedelta(7)\n","    while deltatime > timeincrement:\n","        old_start_dt = start_dt\n","        start_dt = old_start_dt+timeincrement\n","        end_times.append(start_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n","        start_times.append(end_times[-1])\n","        deltatime = end_dt - start_dt\n","    end_times.append(end_time)\n"," \n","    opendap_urls = []\n","    for start_time_seg,end_time_seg in zip(start_times,end_times):\n","        api = GranuleQuery()\n","\n","        # there are three types of searches that I can do.  1) global, 2) bounding box, 3) point radius\n","        if rad_km is None and lon is None and lat is None: # this is a global search\n","            granules = api.short_name(ShortName).version(VersionID).temporal(start_time_seg,end_time_seg).get(1000000) \n","        elif rad_km is None and len(lon) == 2 and len(lat) == 2:\n","            granules = api.short_name(ShortName).version(VersionID).temporal(start_time_seg,end_time_seg).bounding_box(lon[0],lat[0],lon[1],lat[1]).get(1000000)\n","        else: # for now I will assume it is a circle\n","            granules = api.short_name(ShortName).version(VersionID).temporal(start_time_seg,end_time_seg).circle(lon,lat,rad_km*1000.).get(1000000)\n","        \n","        for granule in granules:\n","            for link in granule.get('links',[]):\n","                if 'rel' in link and 'href' in link and 'inherited' not in link:\n","                    if 'http://esipfed.org/ns/fedsearch/1.1/service#' in link['rel'] and 'opendap' in link['href']: # It's an opendap link\n","                        opendap_urls.append(link['href'])\n","            \n","    opendap_urls = list(set(opendap_urls)) # makes them unique and sorts them\n","    opendap_urls.sort() # this will probably put them in chronological order but it must be verified.\n","\n","    return opendap_urls\n"]},{"cell_type":"markdown","metadata":{},"source":["Retrieve URLs and print them"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["['https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200229_B11100Ar_230603200059s.nc4',\n"," 'https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200301_B11100Ar_230603200213s.nc4',\n"," 'https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200302_B11100Ar_230603200315s.nc4']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["urls = get_CMRgranurls(short_name,vid,t1.strftime('%Y-%m-%d'),t2.strftime('%Y.%m.%d'))\n","urls"]},{"cell_type":"markdown","metadata":{},"source":["This is only preparation of data objects in the name space.<br>\n","No storage is allocated here."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["lon = []\n","lat = []\n","time = []\n","xco2 = []\n","qfsmpl = []\n","qcf = []"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Load each granule and prepare for concatenation"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200229_B11100Ar_230603200059s.nc4\n","https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200301_B11100Ar_230603200213s.nc4\n","https://oco2.gesdisc.eosdis.nasa.gov/opendap/OCO2_L2_Lite_FP.11.1r/2020/oco2_LtCO2_200302_B11100Ar_230603200315s.nc4\n"]}],"source":["for i in range(0, len(urls)):\n","    print(urls[i])\n","    fid = nc.Dataset(urls[i])\n","    xco20 = fid.variables['xco2'][:]\n","    lat0 = fid.variables['latitude'][:]\n","    lon0 = fid.variables['longitude'][:]\n","    qcf0 = fid.variables['xco2_quality_flag'][:]\n","    time0 = fid.variables['time'][:]"]},{"cell_type":"markdown","metadata":{},"source":["### 5. Concatenate each variable"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["lon.append(lon0.filled())\n","lat.append(lat0.filled())\n","xco2.append(xco20.filled())\n","qcf.append(qcf0.filled())\n","time.append(time0.filled())"]},{"cell_type":"markdown","metadata":{},"source":["Below, all the variables have been concatenated as 1D vector variables in the computer memory."]},{"cell_type":"markdown","metadata":{},"source":["An example quality screening, based on variable \"xco2_quality_flag\"<br>\n","from the Lite Full-Physics files. This quality flag is stored asvariable \"qcf\".<br>\n","The best quality is when qual=0."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["xco2_all = np.hstack(xco2).squeeze()\n","lon_all  = np.hstack(lon).squeeze()\n","lat_all  = np.hstack(lat).squeeze()\n","qcf_all = np.hstack(qcf).squeeze()\n","time_all = np.hstack(time).squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["Routine to subset best quality data points (qcf==0) out of all data points."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["best = np.where(qcf_all==0)\n","xco2_best = xco2_all[best].squeeze()\n","lon_best = lon_all[best].squeeze()\n","lat_best = lat_all[best].squeeze()\n","time_best = time_all[best].squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["### 6.  Save to a netCDF-4 file\n","It is simplified to the bare minimum to write into your current working directory.<br>\n","Make sure you have enough space.<br>\n","It will create or overwrite netCDF-4 data file \"test.nc\""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["count = len(xco2_best)\n","foutid = nc.Dataset('test.nc',mode='w',format='NETCDF4_CLASSIC') "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["dimtime = foutid.createDimension('time', None)\n","dimlat = foutid.createDimension('lat', count)\n","dimlon = foutid.createDimension('lon', count)\n","dimxco2 = foutid.createDimension('xco2', count)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["varlon = foutid.createVariable('lon',float, ('lon',))\n","varlon.units = 'degrees_east'\n","varlon.long_name = 'longitude'"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["varlat = foutid.createVariable('lat',float, ('lat',))\n","varlat.units = 'degrees_north'\n","varlat.long_names = 'latitude'"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["vartime = foutid.createVariable('time',float, ('time',))\n","vartime.long_name = 'time'"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["varxco2 = foutid.createVariable('xco2',np.float64, ('xco2',))\n","varxco2.units = 'ppm'\n","varxco2.long_name = 'Bias-corrected, quality-filtered XCO2 on X2007 scale'"]},{"cell_type":"markdown","metadata":{},"source":["Fill the Variables"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["varlon[:] = lon_best\n","varlat[:] = lat_best\n","vartime[:] = time_best\n","varxco2[:] = xco2_best "]},{"cell_type":"markdown","metadata":{},"source":["Display the contents of the dataset"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["<class 'netCDF4._netCDF4.Dataset'>\n","root group (NETCDF4_CLASSIC data model, file format HDF5):\n","    dimensions(sizes): time(78975), lat(78975), lon(78975), xco2(78975)\n","    variables(dimensions): float64 lon(lon), float64 lat(lat), float64 time(time), float64 xco2(xco2)\n","    groups: "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["foutid"]},{"cell_type":"markdown","metadata":{},"source":["Close the dataset"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["foutid.close()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":2}
